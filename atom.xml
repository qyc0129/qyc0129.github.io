<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Slow&#39;s River</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2016-10-15T03:00:20.702Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2016/10/15/svd&amp;machine%20learning/"/>
    <id>http://yoursite.com/2016/10/15/svd&amp;machine learning/</id>
    <published>2016-10-15T02:49:28.768Z</published>
    <updated>2016-10-15T03:00:20.702Z</updated>
    
    <content type="html"><![CDATA[<p><strong>svd&amp;machine learning</strong><br>-</p>
<ul>
<li>引言</li>
</ul>
<p>先来看一个例子<br><img src="http://localhost/pagepic/1/1.jpg" alt=""></p>
<p>假设该矩阵表示6名观众对4部电影《肖申克的救赎》、《非常嫌疑犯》、《教父》以及《谋杀绿脚趾》的评分。<br>《教父》看起来获得了最高的平均分1.5分，而《肖申克的救赎》得分较低有0.5分的平均得分。<br>我们进一步将矩阵分解</p>
<p><img src="http://localhost/pagepic/1/2.jpg" alt=""></p>
<p>分别对分解出的三个矩阵命名为A/B/C矩阵。不难看出A矩阵和C矩阵都是由0和1组成的布尔矩阵，B矩阵则是对角矩阵。</p>
<p>C矩阵体现了影片（列）与类型（行）的关系：《肖申克的救赎》与《非常嫌疑犯》分别属于不同题材的电影，即戏剧题材和犯罪题材【ps:个人认为《肖申克的救赎》应该也与《非常嫌疑犯》一样属于犯罪题材，作者举例可能有些不当】，《教父》同时属于两种题材，《谋杀绿脚趾》在属于犯罪题材的同时又引入了新的喜剧题材。</p>
<p>A矩阵表达了这6名观众对电影题材的偏好，观众A/D/E喜爱喜剧题材，观众B/D/E喜爱犯罪题材，观众C/E/F热衷于喜剧。</p>
<p>B矩阵表达了在决定观众对影片的喜好时，犯罪题材所起的作用是其他两种题材的两倍。</p>
<ul>
<li>奇异值</li>
<li>奇异值分解（Singular Value Decomposition）是线性代数中一种重要的矩阵分解，是矩阵分析中正规矩阵酉对角化的推广。在信号处理、统计学等领域有重要应用。<br>A=U<em>R</em>V<br>假设A是一个M <em> N的矩阵，那么得到的U是一个M </em> L的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），R是一个L <em> L的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V 是一个L </em> N的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量）。<br>那么奇异值和特征值是怎么对应起来的呢？首先，我们将一个矩阵A的转置 * A，将会得到一个方阵，我们用这个方阵求特征值可以得到： </li>
</ul>
<p><img src="http://localhost/pagepic/1/3.jpg" alt=""></p>
<p> 这里得到的v，就是右奇异向量。此外</p>
<p><img src="http://localhost/pagepic/1/4.jpg" alt=""></p>
<p>这里的σ就是上面说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前r大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解：</p>
<p><img src="http://localhost/pagepic/1/5.jpg" alt=""></p>
<p>r是一个远小于m、n的数，这样矩阵的乘法看起来像是下面的样子：</p>
<p><img src="http://localhost/pagepic/1/6.jpg" alt=""></p>
<p>后续<br> 在matlab中使用svd&amp;svds函数对第一个矩阵进行奇异值分解，得到的结果和书上编者给出的有较大的差距</p>
<blockquote>
<blockquote>
<p>[U,S,V]=svds(a,3)</p>
</blockquote>
</blockquote>
<p>U =</p>
<p>   -0.1076    0.6224   -0.0327<br>   -0.4933   -0.2291    0.3227<br>   -0.0880   -0.2069   -0.5911<br>   -0.6010    0.3933    0.2901<br>   -0.1956    0.4155   -0.6238<br>   -0.5813   -0.4360   -0.2684</p>
<p>S =</p>
<pre><code>6.9232         0         0
     0    1.9302         0
     0         0    1.1593
</code></pre><p>V =</p>
<p>   -0.1306    0.7414   -0.3160<br>   -0.4840   -0.2816    0.5942<br>   -0.6147    0.4599    0.2782<br>   -0.6090   -0.3994   -0.6853</p>
<p>可以看出奇异值分解得到的矩阵并不是由整数组成的矩阵，可以理解为编者为了让读者比较容易理解而特别设计的一个分解（逆分解合成）的例子。<br>虽然例子可能不成立，但毫无疑问，奇异值分解还是可以给我们对矩阵的分析提供很大的帮助的，从而使我们建立起基于SVD的根据用户偏好进行商品推荐的协同过滤推荐系统。<br>-</p>
<ul>
<li>引用：</li>
</ul>
<p>[1]《机器学习》-PeterFlach</p>
<p>[2] 《强大的矩阵奇异值分解(SVD)及其应用》-LeftNotEasy</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;svd&amp;amp;machine learning&lt;/strong&gt;&lt;br&gt;-&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;引言&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先来看一个例子&lt;br&gt;&lt;img src=&quot;http://localhost/pagepic/1/1.jpg&quot; alt=&quot;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2016/10/14/restart-20161014/"/>
    <id>http://yoursite.com/2016/10/14/restart-20161014/</id>
    <published>2016-10-14T15:46:11.740Z</published>
    <updated>2016-10-14T15:41:49.648Z</updated>
    
    <content type="html"><![CDATA[<p>#Blog Restart<br>在DigitalOcean越来越慢的情况下，今天中午给Centos更换内核后出现了错误，疑似网卡在更换kernel后无法使用，一番折腾后还是各种Module错误，加上DO之前的100刀优惠已经过了一年失效，决定放弃DO的VPS。<br>一番折腾之后换到了Github上挂着博客，全免费还是非常良心的。准备先恢复下之前的文章吧。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;#Blog Restart&lt;br&gt;在DigitalOcean越来越慢的情况下，今天中午给Centos更换内核后出现了错误，疑似网卡在更换kernel后无法使用，一番折腾后还是各种Module错误，加上DO之前的100刀优惠已经过了一年失效，决定放弃DO的VPS。&lt;br&gt;一番
    
    </summary>
    
    
  </entry>
  
</feed>
